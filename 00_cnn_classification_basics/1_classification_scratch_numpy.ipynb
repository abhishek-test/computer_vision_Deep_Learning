{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dataframe = pd.read_csv(\"C:\\\\Abhishek_Data\\\\My_Data\\\\Datasets\\\\Classification\\\\digit-recognizer\\\\train.csv\")\n",
    "print(dataframe.shape)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(dataframe)\n",
    "m,n = data.shape\n",
    "np.random.shuffle(data)  # why random,shuffle-> role while training\n",
    "\n",
    "data_dev = data[0:1000].T\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255. # normalize b/w [0,1] -> why ?\n",
    "\n",
    "train_data = data[1000:m].T\n",
    "Y_train = train_data[0]\n",
    "X_train = train_data[1:n]\n",
    "X_train = X_train / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_init():  \n",
    "\n",
    "    #random initialization -> can we initialize to '0' or constant, if not why ?\n",
    "    # why different initialization for different layers \n",
    "\n",
    "    # he initialization for ReLu\n",
    "    W1 = np.random.randn(10, 784)*np.sqrt(2/784)   \n",
    "    b1 = np.random.randn(10, 1)*np.sqrt(2/784)\n",
    "\n",
    "    # Xavier (normal) iniitalization for SoftMax\n",
    "    W2 = np.random.randn(10, 10)*np.sqrt(2/10)\n",
    "    b2 = np.random.randn(10, 1)*np.sqrt(2/10)\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z) # problems in activation functions -> vanishing/exploding gradient and how to solve them\n",
    "\n",
    "def softmax(Z):\n",
    "    exp = np.exp(Z - np.max(Z)) \n",
    "    return exp / exp.sum(axis=0)\n",
    "\n",
    "def deriv_ReLU(Z):\n",
    "    return Z>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y): # different encoding techniques\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(Y, A2, Z2, W2, A1, Z1, W1, X):\n",
    "    m = Y.size\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = 2*(A2 - one_hot_Y)   # loss functions and types, where to use which one\n",
    "    dW2 = 1/m * dZ2.dot(A1.T)\n",
    "    dB2 = 1/m * np.sum(dZ2, 1)\n",
    "\n",
    "    dZ1 = W2.T.dot(dZ2) * deriv_ReLU(Z1) \n",
    "    dW1 = 1/m * dZ1.dot(X.T)\n",
    "    dB1 = 1/m * np.sum(dZ1, 1)\n",
    "\n",
    "    return dW1, dB1, dW2, dB2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W1, b1, W2, b2, dW1, dB1, dW2, dB2, alpha):   # gradient based update, any queries ??\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * np.reshape(dB1, (10,1))\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * np.reshape(dB2, (10,1))\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, iterations, alpha):\n",
    "    W1, b1, W2, b2 = params_init()\n",
    "\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, dB1, dW2, dB2 = backward_prop(Y, A2, Z2, W2, A1, Z1, W1, X)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, dB1, dW2, dB2, alpha)\n",
    "\n",
    "        if i%50 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            print(\"Accuracy: \", get_accuracy(get_predictions(A2), Y))\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "[5 5 5 ... 5 6 5] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.1031951219512195\n",
      "Iteration:  50\n",
      "[6 9 7 ... 9 7 8] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.6429512195121951\n",
      "Iteration:  100\n",
      "[6 9 7 ... 9 7 8] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.7470243902439024\n",
      "Iteration:  150\n",
      "[6 9 7 ... 9 7 8] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.7952439024390244\n",
      "Iteration:  200\n",
      "[6 9 7 ... 9 7 8] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.8230243902439024\n",
      "Iteration:  250\n",
      "[6 9 7 ... 9 7 8] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.8428292682926829\n",
      "Iteration:  300\n",
      "[6 9 7 ... 9 7 8] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.8521219512195122\n",
      "Iteration:  350\n",
      "[6 9 7 ... 9 7 8] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.8641463414634146\n",
      "Iteration:  400\n",
      "[6 9 7 ... 9 7 8] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.8715853658536585\n",
      "Iteration:  450\n",
      "[6 9 7 ... 9 7 8] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.8782439024390244\n",
      "Iteration:  500\n",
      "[6 9 7 ... 9 7 8] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.8836585365853659\n",
      "Iteration:  550\n",
      "[6 9 7 ... 9 7 8] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.8870243902439024\n",
      "Iteration:  600\n",
      "[6 9 7 ... 9 7 8] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.8904390243902439\n",
      "Iteration:  650\n",
      "[6 9 7 ... 9 7 8] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.8933170731707317\n",
      "Iteration:  700\n",
      "[6 9 7 ... 9 7 8] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.8960243902439025\n",
      "Iteration:  750\n",
      "[6 9 7 ... 9 7 8] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.8988048780487805\n",
      "Iteration:  800\n",
      "[6 9 7 ... 9 7 8] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.901\n",
      "Iteration:  850\n",
      "[6 9 7 ... 9 7 8] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.9029268292682927\n",
      "Iteration:  900\n",
      "[6 9 7 ... 9 7 8] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.9046341463414634\n",
      "Iteration:  950\n",
      "[6 9 7 ... 9 7 8] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.9065121951219512\n",
      "Iteration:  1000\n",
      "[6 9 7 ... 9 7 8] [6 9 7 ... 9 7 8]\n",
      "Accuracy:  0.9086341463414634\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 1001, 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
